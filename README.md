<!-- ðŸ¦™ Llama 3.2 Local Practice (Ollama + Python) -->
<!-- ðŸ“Œ Project Overview -->

This is my first practice project using Llama 3.2 running locally with Ollama.

The project demonstrates:

Connecting to a locally running LLM server

Sending prompts using Python

Handling streaming responses

Parsing JSON responses

Real-time token streaming output

<!-- ðŸ§  Architecture -->

Python Script
â†“
Requests Library
â†“
http://localhost:11434
â†“
Ollama Server
â†“
Llama 3.2 Model
